{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8878868,"sourceType":"datasetVersion","datasetId":5085763}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n# Aim is to predict the marks of students of the test data","metadata":{"id":"-zTLHrFCT6KY","execution":{"iopub.status.busy":"2024-07-06T11:58:26.778051Z","iopub.execute_input":"2024-07-06T11:58:26.779225Z","iopub.status.idle":"2024-07-06T11:58:26.783587Z","shell.execute_reply.started":"2024-07-06T11:58:26.779161Z","shell.execute_reply":"2024-07-06T11:58:26.782501Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Use the file namd 'training data' to train the model\n\ndata = pd.read_excel('/kaggle/input/videos/Training data.xlsx')\nx_train = np.array(data.iloc[:,0:8])\ny_train = np.array(data.iloc[:,8]).reshape(-1,1)\n\n# Try plotting y_train with different features\n# To get an idea whether to add some features or not\n# Add some features if required in x_train\n\n# Also do label encoding for features not represented in numbers\n# refer the link if not know : https://youtu.be/589nCGeWG1w?si=t2Wa7LgbUOO4RooM\n\ndef feature_changing(x_train):\n  le = LabelEncoder()\n  for i in range(x_train.shape[1]):\n      if isinstance(x_train[0, i], str):\n         x_train[:, i] = le.fit_transform(x_train[:, i])\n  return x_train\n\nx_train = feature_changing(x_train)","metadata":{"id":"p0KHq8ZgTpU4","execution":{"iopub.status.busy":"2024-07-06T11:58:26.785663Z","iopub.execute_input":"2024-07-06T11:58:26.785983Z","iopub.status.idle":"2024-07-06T11:58:26.995913Z","shell.execute_reply.started":"2024-07-06T11:58:26.785951Z","shell.execute_reply":"2024-07-06T11:58:26.995215Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def z_score(x_train):\n    x_mean = np.mean(x_train, axis=0)\n    x_std = np.std(x_train, axis=0)\n    x_train = (x_train - x_mean) / x_std\n    return x_train, x_std, x_mean\n\n# Apply z-score normalization\nx_train = x_train.astype(np.float64)\nx_train, x_std, x_mean = z_score(x_train)","metadata":{"id":"tYshvtYlVour","execution":{"iopub.status.busy":"2024-07-06T11:58:26.996839Z","iopub.execute_input":"2024-07-06T11:58:26.997089Z","iopub.status.idle":"2024-07-06T11:58:27.003687Z","shell.execute_reply.started":"2024-07-06T11:58:26.997068Z","shell.execute_reply":"2024-07-06T11:58:27.002802Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def cost(x_train, y_train, w, b):\n  m = len(y_train)\n  y_pred = np.dot(x_train, w) + b\n  loss = np.sum((y_pred - y_train) ** 2) / (2 * m)\n  return loss","metadata":{"id":"O5dOwbNbWJWa","execution":{"iopub.status.busy":"2024-07-06T11:58:27.004926Z","iopub.execute_input":"2024-07-06T11:58:27.005277Z","iopub.status.idle":"2024-07-06T11:58:27.011984Z","shell.execute_reply.started":"2024-07-06T11:58:27.005252Z","shell.execute_reply":"2024-07-06T11:58:27.011157Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Function for gradient descent optimization\ndef gradient_descent(x_train, y_train, w, b, learning_rate=0.01, epochs=1000):\n    m = len(y_train)\n    for epoch in range(epochs):\n        y_pred = np.dot(x_train, w) + b\n        dw = (1 / m) * np.dot(x_train.T, (y_pred - y_train))\n        db = (1 / m) * np.sum(y_pred - y_train)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n    return w, b\n","metadata":{"id":"hW8p2cTNU74W","execution":{"iopub.status.busy":"2024-07-06T11:58:27.014093Z","iopub.execute_input":"2024-07-06T11:58:27.014411Z","iopub.status.idle":"2024-07-06T11:58:27.022808Z","shell.execute_reply.started":"2024-07-06T11:58:27.014375Z","shell.execute_reply":"2024-07-06T11:58:27.022045Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\n\nnp.random.seed(2147483647)\nw = np.random.randn(x_train.shape[1],1)\nb = np.random.randn(1)\n\nold_cost = 0\n\nwhile abs(old_cost - cost(x_train,y_train,w,b))>0.00001:\n  old_cost = cost(x_train,y_train,w,b)\n  w,b = gradient_descent(x_train,y_train,w,b)\n\nx_predict = pd.read_excel('/kaggle/input/videos/Test data.xlsx').iloc[:,:8].to_numpy()\nx_predict = feature_changing(x_predict)\nx_predict = (x_predict - x_mean)/x_std\nans = pd.read_excel('/kaggle/input/videos/Test data.xlsx').iloc[:,8].to_numpy()\n\ny_predict = np.dot(x_predict,w) + b\n\naccuracy = 0\nfor dim in range(len(ans)):\n  if abs(y_predict[dim]-ans[dim])<0.5: # do not change the tolerance as you'll be checked on +- 0.5 error only\n    accuracy += 1\naccuracy = round(accuracy*100/200.0,2)\nok = 'Congratulations' if accuracy>95 else 'Optimization required'\nprint(f\"{ok}, your accuracy is {accuracy}%\")","metadata":{"id":"Kl-fioJ5WkYn","execution":{"iopub.status.busy":"2024-07-06T11:58:27.024695Z","iopub.execute_input":"2024-07-06T11:58:27.024947Z","iopub.status.idle":"2024-07-06T11:58:27.200276Z","shell.execute_reply.started":"2024-07-06T11:58:27.024925Z","shell.execute_reply":"2024-07-06T11:58:27.199397Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Congratulations, your accuracy is 100.0%\n","output_type":"stream"}]}]}